{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac22b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedTensor.shape is (2, 4096, 8, 3, 40)\n",
      "StackedTensor write complete\n",
      "len(slicesList) 3\n",
      "slicesList[0].shape= (2, 4096, 8, 40)\n",
      "slicesList[1].shape= (2, 4096, 8, 40)\n",
      "slicesList[2].shape= (2, 4096, 8, 40)\n",
      "slicesList[0].strides= (15728640, 3840, 480, 4)\n",
      "slicesList[1].strides= (15728640, 3840, 480, 4)\n",
      "slicesList[2].strides= (15728640, 3840, 480, 4)\n",
      "qkv tensor write complete\n",
      "slicesList[0].shape= (2, 8, 4096, 40)\n",
      "slicesList[1].shape= (2, 8, 40, 4096)\n",
      "slicesList[2].shape= (2, 8, 4096, 40)\n",
      "slicesList[0].strides= (15728640, 480, 3840, 4)\n",
      "slicesList[1].strides= (15728640, 480, 4, 3840)\n",
      "slicesList[2].strides= (15728640, 480, 3840, 4)\n",
      "qkv transposed tensor write complete\n"
     ]
    }
   ],
   "source": [
    "#This block splits a stacked 5d tensor StackedQueryKeyValue to q,k,v tensors and transpose them. \n",
    "#Both untransposed and transposed q,k,v tensors are dumped\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "#create horizontally stacked Tensor\n",
    "batchSize = 2\n",
    "headSize = 40\n",
    "sequenceLength = 4096\n",
    "headCount = 8\n",
    "#When you receive StackedQueryKeyValue, it will have a shape of {batchSize, sequenceLength, numHeads, 3, headSize}\n",
    "stackedTensorSize = batchSize*sequenceLength*headCount*3*headSize\n",
    "stackedTensor = np.arange(stackedTensorSize).reshape(batchSize, sequenceLength, headCount, 3, headSize)\n",
    "print(\"stackedTensor.shape is\",stackedTensor.shape)\n",
    "stackedTensor.tofile('stackedTensor.txt',sep=\" \",format=\"%s\")\n",
    "print(\"StackedTensor write complete\")\n",
    "\n",
    "#You can then split that tensor on the 4th dimension into 3 tensors with the same shape \n",
    "#{batchSize, sequenceLength, numHeads, 1, headSize} \n",
    "#(Query is the first tensor, Key is the second tensor and Value is the third tensor)\n",
    "\n",
    "#split this stackedtensor along 4th dim\n",
    "slicesList =  np.array_split(stackedTensor,3,3)\n",
    "print(\"len(slicesList)\", len(slicesList)) #returns 3\n",
    "#sliceList contains the q,k,v tensors\n",
    "#print(slicesList[0].shape)\n",
    "#slicesList[0] = slicesList[0].reshape(batchSize, sequenceLength, headCount, headSize)\n",
    "#print(slicesList[0].shape)\n",
    "\n",
    "slicesList[0] = slicesList[0].reshape(batchSize, sequenceLength, headCount, headSize)\n",
    "slicesList[1] = slicesList[1].reshape(batchSize, sequenceLength, headCount, headSize)\n",
    "slicesList[2] = slicesList[2].reshape(batchSize, sequenceLength, headCount, headSize)\n",
    "print(\"slicesList[0].shape=\",slicesList[0].shape)\n",
    "print(\"slicesList[1].shape=\",slicesList[1].shape)\n",
    "print(\"slicesList[2].shape=\",slicesList[2].shape)\n",
    "\n",
    "print(\"slicesList[0].strides=\",slicesList[0].strides)\n",
    "print(\"slicesList[1].strides=\",slicesList[1].strides)\n",
    "print(\"slicesList[2].strides=\",slicesList[2].strides)\n",
    "\n",
    "slicesList[0].tofile('qkv_QTensor',sep=\" \",format=\"%s\")\n",
    "slicesList[1].tofile('qkv_KTensor',sep=\" \",format=\"%s\")\n",
    "slicesList[2].tofile('qkv_VTensor',sep=\" \",format=\"%s\")\n",
    "\n",
    "print(\"qkv tensor write complete\")\n",
    "\n",
    "#Those tensors can then be easily reshaped to 4D tensors of shape {batchSize, sequenceLength, headCount, headSize}\n",
    "#4.\tThen, you need to transpose the query tensor to the {batchSize, headCount, sequenceLength, headSize} shape\n",
    "#5.\tYou also need to transpose the key tensor to the {batchSize, headCount, headSize, kvSequenceLength} shape\n",
    "#6.\tYou also need to transpose the value tensor to the {batchSize, headCount, kvSequenceLength, vHeadSize} shape\n",
    "\n",
    "#Transpose\n",
    "\n",
    "#Those tensors can then be easily reshaped to 4D tensors of shape {batchSize, sequenceLength, headCount, headSize} 0,1,2,3\n",
    "#Then, you need to transpose the query tensor to the {batchSize, headCount, sequenceLength, headSize} shape 0,2,1,3\n",
    "#You also need to transpose the key tensor to the {batchSize, headCount, headSize, kvSequenceLength} shape 0,2,3,1\n",
    "#You also need to transpose the value tensor to the {batchSize, headCount, kvSequenceLength, vHeadSize} shape 0,2,1,3\n",
    "\n",
    "slicesList[0] = slicesList[0].transpose(0,2,1,3)\n",
    "slicesList[1] = slicesList[1].transpose(0,2,3,1)\n",
    "slicesList[2] = slicesList[2].transpose(0,2,1,3)\n",
    "\n",
    "slicesList[0].tofile('qkv_QTensorTranspose',sep=\" \",format=\"%s\")\n",
    "slicesList[1].tofile('qkv_KTensorTranspose',sep=\" \",format=\"%s\")\n",
    "slicesList[2].tofile('qkv_VTensorTranspose',sep=\" \",format=\"%s\")\n",
    "\n",
    "print(\"slicesList[0].shape=\",slicesList[0].shape)\n",
    "print(\"slicesList[1].shape=\",slicesList[1].shape)\n",
    "print(\"slicesList[2].shape=\",slicesList[2].shape)\n",
    "\n",
    "print(\"slicesList[0].strides=\",slicesList[0].strides)\n",
    "print(\"slicesList[1].strides=\",slicesList[1].strides)\n",
    "print(\"slicesList[2].strides=\",slicesList[2].strides)\n",
    "\n",
    "print(\"qkv transposed tensor write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7b2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedTensor.shape is (2, 64, 8, 2, 160)\n",
      "StackedTensor write complete\n",
      "len(slicesList) 2\n",
      "slicesList[0].shape= (2, 64, 8, 160)\n",
      "slicesList[1].shape= (2, 64, 8, 160)\n",
      "slicesList[0].strides= (655360, 10240, 1280, 4)\n",
      "slicesList[1].strides= (655360, 10240, 1280, 4)\n",
      "kv tensor write complete\n",
      "kv transposed tensor write complete\n"
     ]
    }
   ],
   "source": [
    "#This block splits a stacked 5d tensor StackedKeyValue to k,v tensors and transpose them. \n",
    "#Both untransposed and transposed k,v tensors are dumped\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "batchSize = 2\n",
    "kvSequenceLength = 64\n",
    "headCount = 8\n",
    "headSize = 160\n",
    "\n",
    "#When you receive StackedKeyValue, it will have a shape of {batchSize, kvSequenceLength, numHeads, 2, headSize}\n",
    "stackedTensorSize = batchSize*kvSequenceLength*headCount*2*headSize\n",
    "stackedTensor = np.arange(stackedTensorSize).reshape(batchSize, kvSequenceLength, headCount, 2, headSize)\n",
    "print(\"stackedTensor.shape is\",stackedTensor.shape)\n",
    "stackedTensor.tofile('stackedTensor.txt',sep=\" \",format=\"%s\")\n",
    "print(\"StackedTensor write complete\")\n",
    "\n",
    "#You can then split that tensor on the 4th dimension into 2 tensors with the same shape \n",
    "#{batchSize, kvSequenceLength, numHeads, 1, headSize} \n",
    "#(Key is the first tensor, Value is the second tensor )\n",
    "\n",
    "#split this stackedtensor along 4th dim\n",
    "slicesList =  np.array_split(stackedTensor,2,3)\n",
    "print(\"len(slicesList)\", len(slicesList)) #returns 2\n",
    "\n",
    "\n",
    "slicesList[0] = slicesList[0].reshape(batchSize, kvSequenceLength, headCount, headSize)\n",
    "slicesList[1] = slicesList[1].reshape(batchSize, kvSequenceLength, headCount, headSize)\n",
    "\n",
    "print(\"slicesList[0].shape=\",slicesList[0].shape)\n",
    "print(\"slicesList[1].shape=\",slicesList[1].shape)\n",
    "print(\"slicesList[0].strides=\",slicesList[0].strides)\n",
    "print(\"slicesList[1].strides=\",slicesList[1].strides)\n",
    "\n",
    "slicesList[0].tofile('kv_KTensor',sep=\" \",format=\"%s\")\n",
    "slicesList[1].tofile('kv_VTensor',sep=\" \",format=\"%s\")\n",
    "\n",
    "print(\"kv tensor write complete\")\n",
    "\n",
    "#Those tensors can then be easily reshaped to 4D tensors of shape {batchSize, kvSequenceLength, headCount, headSize) 0,1,2,3\n",
    "#Then, you need to transpose the key tensor to the {batchSize, headCount, headSize, kvSequenceLength} shape 0,2,3,1\n",
    "#You also need to transpose the value tensor to the {batchSize, headCount, kvSequenceLength, vHeadSize} shape 0,2,1,3\n",
    "\n",
    "slicesList[0] = slicesList[0].transpose(0,2,3,1) #key indices are moved twice: double transpose\n",
    "slicesList[1] = slicesList[1].transpose(0,2,1,3) #value\n",
    "\n",
    "slicesList[0].tofile('kv_KTensorTranspose',sep=\" \",format=\"%s\")\n",
    "slicesList[1].tofile('kv_VTensorTranspose',sep=\" \",format=\"%s\")\n",
    "\n",
    "print(\"kv transposed tensor write complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d7da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTensor.shape is (2, 256, 8, 160)\n",
      "QTensorBeforeTranspose write complete\n",
      "QTensorTranspose.shape is (2, 8, 256, 160)\n",
      "QTensorTranspose.strides is (1310720, 640, 5120, 4)\n",
      "QTensorAfterTranspose write complete\n"
     ]
    }
   ],
   "source": [
    "#5d QTensor transpose\n",
    "#6.\tAs for the Query tensor, it will have a shape of {batchSize, sequenceLength, hiddenSize}. \n",
    "#This can be reshaped to a 4D tensor of shape {batchSize, sequenceLength, headCount, headSize}  0,1,2,3,\n",
    "# and then transposed to a tensor of shape {batchSize, headCount, sequenceLength, headSize} 0,2,1,3\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "batchSize = 2\n",
    "QsequenceLength = 256\n",
    "hiddenSize = 1280\n",
    "headCount = 8\n",
    "headSize = 160\n",
    "\n",
    "QTensorSize = batchSize*QsequenceLength*headCount*1*headSize\n",
    "QTensor = np.arange(QTensorSize).reshape(batchSize, QsequenceLength, headCount, headSize) # working\n",
    "#QTensor = np.arange(QTensorSize).reshape(batchSize, QsequenceLength, headCount*headSize)\n",
    "#QTensor = np.arange(QTensorSize).reshape(1,1,batchSize, QsequenceLength, hiddenSize)\n",
    "#QTensor = np.reshape(batchSize, QsequenceLength, headCount, headSize)\n",
    "print(\"QTensor.shape is\",QTensor.shape)\n",
    "QTensor.tofile('QTensorBeforeTranspose',sep=\" \",format=\"%s\")\n",
    "print(\"QTensorBeforeTranspose write complete\")\n",
    "QTensorTranspose = QTensor.transpose(0,2,1,3)\n",
    "print(\"QTensorTranspose.shape is\",QTensorTranspose.shape)\n",
    "print(\"QTensorTranspose.strides is\",QTensorTranspose.strides)\n",
    "QTensorTranspose.tofile('QTensorAfterTranspose',sep=\" \",format=\"%s\")\n",
    "print(\"QTensorAfterTranspose write complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03b5d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]]\n",
      "\n",
      "\n",
      " [[[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]\n",
      "\n",
      "  [[0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   ...\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]\n",
      "   [0.015625 0.015625 0.015625 ... 0.015625 0.015625 0.015625]]]]\n"
     ]
    }
   ],
   "source": [
    "#Tensor softmax using dimensions of output of Gemm(q,k) thats of shape \n",
    "#{batchSize, headCount, sequenceLength, kvSequenceLength}\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "#batchSize = 2\n",
    "#headCount = 8\n",
    "#sequenceLength = 256\n",
    "#kvSequenceLength = 64\n",
    "batchSize = 2\n",
    "headCount = 8\n",
    "sequenceLength = 256\n",
    "kvSequenceLength = 64\n",
    "\n",
    "def softmax(z):\n",
    "    #assert len(z.shape) == 2\n",
    "    s = np.max(z, axis=3)\n",
    "    s = s[:, :, : ,np.newaxis] # necessary step to do broadcasting\n",
    "    e_x = np.exp(z - s)\n",
    "    div = np.sum(e_x, axis=3)\n",
    "    div = div[:, :, :, np.newaxis] # dito\n",
    "    return e_x / div\n",
    "\n",
    "inTensorSize = batchSize*headCount*sequenceLength*kvSequenceLength\n",
    "inTensor = np.full((batchSize, headCount, sequenceLength, kvSequenceLength), 3)\n",
    "print(softmax(inTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e346bd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MhaGemm1OutTensor.shape is (2, 8, 4096, 40)\n",
      "MhaGemm1OutTensor write complete\n",
      "MhaGemm1OutTensorTranspose.shape is (2, 4096, 8, 40)\n",
      "MhaGemm1OutTensorTranspose write complete\n"
     ]
    }
   ],
   "source": [
    "#9.After softmax, it’s time to do the second GEMM, which is a dot product of the result of Softmax and the Value tensor. \n",
    "#The result of this GEMM will have shape {batchSize, headCount, sequenceLength, vHeadSize}\n",
    "#10.Then, transpose the result of the second GEMM into a tensor that has shape {batchSize, sequenceLength, headCount, vHeadSize}\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "batchSize = 2\n",
    "headCount = 8\n",
    "sequenceLength = 4096\n",
    "vheadSize = 40\n",
    "\n",
    "MhaGemm1OutTensorSize = batchSize*sequenceLength*headCount*vheadSize\n",
    "MhaGemm1OutTensor = np.arange(MhaGemm1OutTensorSize).reshape(batchSize, headCount, sequenceLength, vheadSize) #0,1,2,3\n",
    "print(\"MhaGemm1OutTensor.shape is\",MhaGemm1OutTensor.shape)\n",
    "MhaGemm1OutTensor.tofile('MhaGemm1OutTensor',sep=\" \",format=\"%s\")\n",
    "print(\"MhaGemm1OutTensor write complete\")\n",
    "MhaGemm1OutTensorTranspose = MhaGemm1OutTensor.transpose(0,2,1,3) #swap headcount and seqlen indices\n",
    "print(\"MhaGemm1OutTensorTranspose.shape is\",MhaGemm1OutTensorTranspose.shape)\n",
    "MhaGemm1OutTensorTranspose.tofile('MhaGemm1OutTensorTranspose',sep=\" \",format=\"%s\")\n",
    "print(\"MhaGemm1OutTensorTranspose write complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b695cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qTensor.shape= (2, 8, 256, 160)\n",
      "kTensor.shape= (2, 8, 160, 64)\n",
      "qTensor= [[[[     0      1      2 ...    157    158    159]\n",
      "   [   160    161    162 ...    317    318    319]\n",
      "   [   320    321    322 ...    477    478    479]\n",
      "   ...\n",
      "   [ 40480  40481  40482 ...  40637  40638  40639]\n",
      "   [ 40640  40641  40642 ...  40797  40798  40799]\n",
      "   [ 40800  40801  40802 ...  40957  40958  40959]]\n",
      "\n",
      "  [[ 40960  40961  40962 ...  41117  41118  41119]\n",
      "   [ 41120  41121  41122 ...  41277  41278  41279]\n",
      "   [ 41280  41281  41282 ...  41437  41438  41439]\n",
      "   ...\n",
      "   [ 81440  81441  81442 ...  81597  81598  81599]\n",
      "   [ 81600  81601  81602 ...  81757  81758  81759]\n",
      "   [ 81760  81761  81762 ...  81917  81918  81919]]\n",
      "\n",
      "  [[ 81920  81921  81922 ...  82077  82078  82079]\n",
      "   [ 82080  82081  82082 ...  82237  82238  82239]\n",
      "   [ 82240  82241  82242 ...  82397  82398  82399]\n",
      "   ...\n",
      "   [122400 122401 122402 ... 122557 122558 122559]\n",
      "   [122560 122561 122562 ... 122717 122718 122719]\n",
      "   [122720 122721 122722 ... 122877 122878 122879]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[204800 204801 204802 ... 204957 204958 204959]\n",
      "   [204960 204961 204962 ... 205117 205118 205119]\n",
      "   [205120 205121 205122 ... 205277 205278 205279]\n",
      "   ...\n",
      "   [245280 245281 245282 ... 245437 245438 245439]\n",
      "   [245440 245441 245442 ... 245597 245598 245599]\n",
      "   [245600 245601 245602 ... 245757 245758 245759]]\n",
      "\n",
      "  [[245760 245761 245762 ... 245917 245918 245919]\n",
      "   [245920 245921 245922 ... 246077 246078 246079]\n",
      "   [246080 246081 246082 ... 246237 246238 246239]\n",
      "   ...\n",
      "   [286240 286241 286242 ... 286397 286398 286399]\n",
      "   [286400 286401 286402 ... 286557 286558 286559]\n",
      "   [286560 286561 286562 ... 286717 286718 286719]]\n",
      "\n",
      "  [[286720 286721 286722 ... 286877 286878 286879]\n",
      "   [286880 286881 286882 ... 287037 287038 287039]\n",
      "   [287040 287041 287042 ... 287197 287198 287199]\n",
      "   ...\n",
      "   [327200 327201 327202 ... 327357 327358 327359]\n",
      "   [327360 327361 327362 ... 327517 327518 327519]\n",
      "   [327520 327521 327522 ... 327677 327678 327679]]]\n",
      "\n",
      "\n",
      " [[[327680 327681 327682 ... 327837 327838 327839]\n",
      "   [327840 327841 327842 ... 327997 327998 327999]\n",
      "   [328000 328001 328002 ... 328157 328158 328159]\n",
      "   ...\n",
      "   [368160 368161 368162 ... 368317 368318 368319]\n",
      "   [368320 368321 368322 ... 368477 368478 368479]\n",
      "   [368480 368481 368482 ... 368637 368638 368639]]\n",
      "\n",
      "  [[368640 368641 368642 ... 368797 368798 368799]\n",
      "   [368800 368801 368802 ... 368957 368958 368959]\n",
      "   [368960 368961 368962 ... 369117 369118 369119]\n",
      "   ...\n",
      "   [409120 409121 409122 ... 409277 409278 409279]\n",
      "   [409280 409281 409282 ... 409437 409438 409439]\n",
      "   [409440 409441 409442 ... 409597 409598 409599]]\n",
      "\n",
      "  [[409600 409601 409602 ... 409757 409758 409759]\n",
      "   [409760 409761 409762 ... 409917 409918 409919]\n",
      "   [409920 409921 409922 ... 410077 410078 410079]\n",
      "   ...\n",
      "   [450080 450081 450082 ... 450237 450238 450239]\n",
      "   [450240 450241 450242 ... 450397 450398 450399]\n",
      "   [450400 450401 450402 ... 450557 450558 450559]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[532480 532481 532482 ... 532637 532638 532639]\n",
      "   [532640 532641 532642 ... 532797 532798 532799]\n",
      "   [532800 532801 532802 ... 532957 532958 532959]\n",
      "   ...\n",
      "   [572960 572961 572962 ... 573117 573118 573119]\n",
      "   [573120 573121 573122 ... 573277 573278 573279]\n",
      "   [573280 573281 573282 ... 573437 573438 573439]]\n",
      "\n",
      "  [[573440 573441 573442 ... 573597 573598 573599]\n",
      "   [573600 573601 573602 ... 573757 573758 573759]\n",
      "   [573760 573761 573762 ... 573917 573918 573919]\n",
      "   ...\n",
      "   [613920 613921 613922 ... 614077 614078 614079]\n",
      "   [614080 614081 614082 ... 614237 614238 614239]\n",
      "   [614240 614241 614242 ... 614397 614398 614399]]\n",
      "\n",
      "  [[614400 614401 614402 ... 614557 614558 614559]\n",
      "   [614560 614561 614562 ... 614717 614718 614719]\n",
      "   [614720 614721 614722 ... 614877 614878 614879]\n",
      "   ...\n",
      "   [654880 654881 654882 ... 655037 655038 655039]\n",
      "   [655040 655041 655042 ... 655197 655198 655199]\n",
      "   [655200 655201 655202 ... 655357 655358 655359]]]]\n",
      "kTensor= [[[[     0      1      2 ...     61     62     63]\n",
      "   [    64     65     66 ...    125    126    127]\n",
      "   [   128    129    130 ...    189    190    191]\n",
      "   ...\n",
      "   [ 10048  10049  10050 ...  10109  10110  10111]\n",
      "   [ 10112  10113  10114 ...  10173  10174  10175]\n",
      "   [ 10176  10177  10178 ...  10237  10238  10239]]\n",
      "\n",
      "  [[ 10240  10241  10242 ...  10301  10302  10303]\n",
      "   [ 10304  10305  10306 ...  10365  10366  10367]\n",
      "   [ 10368  10369  10370 ...  10429  10430  10431]\n",
      "   ...\n",
      "   [ 20288  20289  20290 ...  20349  20350  20351]\n",
      "   [ 20352  20353  20354 ...  20413  20414  20415]\n",
      "   [ 20416  20417  20418 ...  20477  20478  20479]]\n",
      "\n",
      "  [[ 20480  20481  20482 ...  20541  20542  20543]\n",
      "   [ 20544  20545  20546 ...  20605  20606  20607]\n",
      "   [ 20608  20609  20610 ...  20669  20670  20671]\n",
      "   ...\n",
      "   [ 30528  30529  30530 ...  30589  30590  30591]\n",
      "   [ 30592  30593  30594 ...  30653  30654  30655]\n",
      "   [ 30656  30657  30658 ...  30717  30718  30719]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 51200  51201  51202 ...  51261  51262  51263]\n",
      "   [ 51264  51265  51266 ...  51325  51326  51327]\n",
      "   [ 51328  51329  51330 ...  51389  51390  51391]\n",
      "   ...\n",
      "   [ 61248  61249  61250 ...  61309  61310  61311]\n",
      "   [ 61312  61313  61314 ...  61373  61374  61375]\n",
      "   [ 61376  61377  61378 ...  61437  61438  61439]]\n",
      "\n",
      "  [[ 61440  61441  61442 ...  61501  61502  61503]\n",
      "   [ 61504  61505  61506 ...  61565  61566  61567]\n",
      "   [ 61568  61569  61570 ...  61629  61630  61631]\n",
      "   ...\n",
      "   [ 71488  71489  71490 ...  71549  71550  71551]\n",
      "   [ 71552  71553  71554 ...  71613  71614  71615]\n",
      "   [ 71616  71617  71618 ...  71677  71678  71679]]\n",
      "\n",
      "  [[ 71680  71681  71682 ...  71741  71742  71743]\n",
      "   [ 71744  71745  71746 ...  71805  71806  71807]\n",
      "   [ 71808  71809  71810 ...  71869  71870  71871]\n",
      "   ...\n",
      "   [ 81728  81729  81730 ...  81789  81790  81791]\n",
      "   [ 81792  81793  81794 ...  81853  81854  81855]\n",
      "   [ 81856  81857  81858 ...  81917  81918  81919]]]\n",
      "\n",
      "\n",
      " [[[ 81920  81921  81922 ...  81981  81982  81983]\n",
      "   [ 81984  81985  81986 ...  82045  82046  82047]\n",
      "   [ 82048  82049  82050 ...  82109  82110  82111]\n",
      "   ...\n",
      "   [ 91968  91969  91970 ...  92029  92030  92031]\n",
      "   [ 92032  92033  92034 ...  92093  92094  92095]\n",
      "   [ 92096  92097  92098 ...  92157  92158  92159]]\n",
      "\n",
      "  [[ 92160  92161  92162 ...  92221  92222  92223]\n",
      "   [ 92224  92225  92226 ...  92285  92286  92287]\n",
      "   [ 92288  92289  92290 ...  92349  92350  92351]\n",
      "   ...\n",
      "   [102208 102209 102210 ... 102269 102270 102271]\n",
      "   [102272 102273 102274 ... 102333 102334 102335]\n",
      "   [102336 102337 102338 ... 102397 102398 102399]]\n",
      "\n",
      "  [[102400 102401 102402 ... 102461 102462 102463]\n",
      "   [102464 102465 102466 ... 102525 102526 102527]\n",
      "   [102528 102529 102530 ... 102589 102590 102591]\n",
      "   ...\n",
      "   [112448 112449 112450 ... 112509 112510 112511]\n",
      "   [112512 112513 112514 ... 112573 112574 112575]\n",
      "   [112576 112577 112578 ... 112637 112638 112639]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[133120 133121 133122 ... 133181 133182 133183]\n",
      "   [133184 133185 133186 ... 133245 133246 133247]\n",
      "   [133248 133249 133250 ... 133309 133310 133311]\n",
      "   ...\n",
      "   [143168 143169 143170 ... 143229 143230 143231]\n",
      "   [143232 143233 143234 ... 143293 143294 143295]\n",
      "   [143296 143297 143298 ... 143357 143358 143359]]\n",
      "\n",
      "  [[143360 143361 143362 ... 143421 143422 143423]\n",
      "   [143424 143425 143426 ... 143485 143486 143487]\n",
      "   [143488 143489 143490 ... 143549 143550 143551]\n",
      "   ...\n",
      "   [153408 153409 153410 ... 153469 153470 153471]\n",
      "   [153472 153473 153474 ... 153533 153534 153535]\n",
      "   [153536 153537 153538 ... 153597 153598 153599]]\n",
      "\n",
      "  [[153600 153601 153602 ... 153661 153662 153663]\n",
      "   [153664 153665 153666 ... 153725 153726 153727]\n",
      "   [153728 153729 153730 ... 153789 153790 153791]\n",
      "   ...\n",
      "   [163648 163649 163650 ... 163709 163710 163711]\n",
      "   [163712 163713 163714 ... 163773 163774 163775]\n",
      "   [163776 163777 163778 ... 163837 163838 163839]]]]\n",
      "gemm result= [[[[   86563840    86576560    86589280 ...    87339760    87352480\n",
      "       87365200]\n",
      "   [  216816640   216854960   216893280 ...   219154160   219192480\n",
      "      219230800]\n",
      "   [  347069440   347133360   347197280 ...   350968560   351032480\n",
      "      351096400]\n",
      "   ...\n",
      "   [-1319216128 -1312726608 -1306237088 ...  -923355408  -916865888\n",
      "     -910376368]\n",
      "   [-1188963328 -1182448208 -1175933088 ...  -791541008  -785025888\n",
      "     -778510768]\n",
      "   [-1058710528 -1052169808 -1045629088 ...  -659726608  -653185888\n",
      "     -646645168]]\n",
      "\n",
      "  [[ 1886149632  1892715952  1899282272 ... -2008272144 -2001705824\n",
      "    -1995139504]\n",
      "   [-2016420864 -2009828944 -2003237024 ... -1614313744 -1607721824\n",
      "    -1601129904]\n",
      "   [-1624024064 -1617406544 -1610789024 ... -1220355344 -1213737824\n",
      "    -1207120304]\n",
      "   ...\n",
      "   [-1916675072 -1903631952 -1890588832 ... -1121044752 -1108001632\n",
      "    -1094958512]\n",
      "   [-1524278272 -1511209552 -1498140832 ...  -727086352  -714017632\n",
      "     -700948912]\n",
      "   [-1131881472 -1118787152 -1105692832 ...  -333127952  -320033632\n",
      "     -306939312]]\n",
      "\n",
      "  [[  464509952   477629872   490749792 ...  1264825072  1277944992\n",
      "     1291064912]\n",
      "   [ 1119050752  1132196272  1145341792 ...  1920927472  1934072992\n",
      "     1947218512]\n",
      "   [ 1773591552  1786762672  1799933792 ... -1717937424 -1704766304\n",
      "    -1691595184]\n",
      "   ...\n",
      "   [-1440392192 -1420795472 -1401198752 ...  -244992272  -225395552\n",
      "     -205798832]\n",
      "   [ -785851392  -766229072  -746606752 ...   411110128   430732448\n",
      "      450354768]\n",
      "   [ -131310592  -111662672   -92014752 ...  1067212528  1086860448\n",
      "     1106508368]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1652925440 -1620144720 -1587364000 ...   346698480   379479200\n",
      "      412259920]\n",
      "   [ -211952640  -179146320  -146340000 ...  1789232880  1822039200\n",
      "     1854845520]\n",
      "   [ 1229020160  1261852080  1294684000 ... -1063200016 -1030368096\n",
      "     -997536176]\n",
      "   ...\n",
      "   [ 2135940096 -2119769680 -2080512160 ...   235681520   274939040\n",
      "      314196560]\n",
      "   [ -718054400  -678771280  -639488160 ...  1678215920  1717499040\n",
      "     1756782160]\n",
      "   [  722918400   762227120   801535840 ... -1174216976 -1134908256\n",
      "    -1095599536]]\n",
      "\n",
      "  [[ 1220402176  1259736496  1299070816 ...  -675171600  -635837280\n",
      "     -596502960]\n",
      "   [-1371448320 -1332088400 -1292728480 ...  1029506800  1068866720\n",
      "     1108226640]\n",
      "   [  331668480   371054000   410439520 ... -1560782096 -1521396576\n",
      "    -1482011056]\n",
      "   ...\n",
      "   [-1682744320 -1636933200 -1591122080 ...  1111734000  1157545120\n",
      "     1203356240]\n",
      "   [   20372480    66209200   112045920 ... -1478554896 -1432718176\n",
      "    -1386881456]\n",
      "   [ 1723489280  1769351600  1815213920 ...   226123504   271985824\n",
      "      317848144]]\n",
      "\n",
      "  [[  872504320   918392240   964280160 ...  -623299856  -577411936\n",
      "     -531524016]\n",
      "   [-1457202176 -1411288656 -1365375136 ...  1343522544  1389436064\n",
      "     1435349584]\n",
      "   [  508058624   553997744   599936864 ...  -984622352  -938683232\n",
      "     -892744112]\n",
      "   ...\n",
      "   [ -132719616   -80354896   -27990176 ... -1233438992 -1181074272\n",
      "    -1128709552]\n",
      "   [ 1832541184  1884931504  1937321824 ...   733383408   785773728\n",
      "      838164048]\n",
      "   [ -497165312  -444749392  -392333472 ... -1594761488 -1542345568\n",
      "    -1489929648]]]\n",
      "\n",
      "\n",
      " [[[ 1598348288  1650789808  1703231328 ...   502313712   554755232\n",
      "      607196752]\n",
      "   [ -469214208  -416747088  -364279968 ... -1563687184 -1511220064\n",
      "    -1458752944]\n",
      "   [ 1758190592  1810683312  1863176032 ...   665279216   717771936\n",
      "      770264656]\n",
      "   ...\n",
      "   [-1803920384 -1745002064 -1686083744 ...  1790097136  1849015456\n",
      "     1907933776]\n",
      "   [  423484416   482428336   541372256 ...  -275903760  -216959840\n",
      "     -158015920]\n",
      "   [-1644078080 -1585108560 -1526139040 ...  1953062640  2012032160\n",
      "     2071001680]]\n",
      "\n",
      "  [[ -897033216  -838038096  -779042976 ... -1593298192 -1534303072\n",
      "    -1475307952]\n",
      "   [ 1592515584  1651536304  1710557024 ...   897812208   956832928\n",
      "     1015853648]\n",
      "   [ -212902912  -153856592   -94810272 ...  -906044688  -846998368\n",
      "     -787952048]\n",
      "   ...\n",
      "   [ 1893587968  1959059888  2024531808 ...  1592407792  1657879712\n",
      "     1723351632]\n",
      "   [   88169472   153666992   219164512 ...  -211449104  -145951584\n",
      "      -80454064]\n",
      "   [-1717249024 -1651725904 -1586202784 ... -2015306000 -1949782880\n",
      "    -1884259760]]\n",
      "\n",
      "  [[ 1976294400  2041843120  2107391840 ...  1679799024  1745347744\n",
      "     1810896464]\n",
      "   [  433019904   498594224   564168544 ...   138086128   203660448\n",
      "      269234768]\n",
      "   [-1110254592 -1044654672  -979054752 ... -1403626768 -1338026848\n",
      "    -1272426928]\n",
      "   ...\n",
      "   [-1925096448 -1853070928 -1781045408 ... -1826507024 -1754481504\n",
      "    -1682455984]\n",
      "   [  826596352   898647472   970698592 ...   926747376   998798496\n",
      "     1070849616]\n",
      "   [ -716678144  -644601424  -572524704 ...  -614965520  -542888800\n",
      "     -470812080]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ -141140992   -55931472    29278048 ...   761672432   846881952\n",
      "      932091472]\n",
      "   [ -897983488  -812748368  -727513248 ...     6391536    91626656\n",
      "      176861776]\n",
      "   [-1654825984 -1569565264 -1484304544 ...  -748889360  -663628640\n",
      "     -578367920]\n",
      "   ...\n",
      "   [ 1651235840  1742922160  1834608480 ... -1345833232 -1254146912\n",
      "    -1162460592]\n",
      "   [  894393344   986105264  1077817184 ... -2101114128 -2009402208\n",
      "    -1917690288]\n",
      "   [  137550848   229288368   321025888 ...  1438572272  1530309792\n",
      "     1622047312]]\n",
      "\n",
      "  [[-1562780672 -1471017552 -1379254432 ...  -260197648  -168434528\n",
      "      -76671408]\n",
      "   [-2057479168 -1965690448 -1873901728 ...  -753334544  -661545824\n",
      "     -569757104]\n",
      "   [ 1742789632  1834603952  1926418272 ... -1246471440 -1154657120\n",
      "    -1062842800]\n",
      "   ...\n",
      "   [ 2127518720 -2069208656 -1970968736 ...  -469780752  -371540832\n",
      "     -273300912]\n",
      "   [ 1632820224  1731085744  1829351264 ...  -962917648  -864652128\n",
      "     -766386608]\n",
      "   [ 1138121728  1236412848  1334703968 ... -1456054544 -1357763424\n",
      "    -1259472304]]\n",
      "\n",
      "  [[-1910678528 -1812361808 -1714045088 ...  -208325904  -110009184\n",
      "      -11692464]\n",
      "   [-2143233024 -2044890704 -1946548384 ...  -439318800  -340976480\n",
      "     -242634160]\n",
      "   [ 1919179776  2017547696  2115915616 ...  -670311696  -571943776\n",
      "     -473575856]\n",
      "   ...\n",
      "   [ -617423872  -512630352  -407836832 ...  1480013552  1584807072\n",
      "     1689600592]\n",
      "   [ -849978368  -745159248  -640340128 ...  1249020656  1353839776\n",
      "     1458658896]\n",
      "   [-1082532864  -977688144  -872843424 ...  1018027760  1122872480\n",
      "     1227717200]]]]\n",
      "(2, 8, 256, 64)\n"
     ]
    }
   ],
   "source": [
    "#int q_sizes[dim_count] = {2,1,2,32}; //{batch, headcount, seqlen,headsize } = //{batch, headcount, M,K}\n",
    "#int k_sizes[dim_count] = {2,1,32,2 }; //{batch, headcount,headsize, kvseqlen } = ////{batch, headcount,K,N}\n",
    "#int out_sizes[dim_count] = { 2,1,2,2 }; // {batchSize, headCount, sequenceLength, kvSequenceLength}\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "#Gemm using matmul\n",
    "#{2,1,2,32}\n",
    "#2,8,256,160\n",
    "#int q_sizes[dim_count] = { 2,8,256,160 }; //{batch, headcount, seqlen,headsize } = //{batch, headcount, M,K}\n",
    "#int k_sizes[dim_count] = { 2,8,160,64 }; //{batch, headcount, headsize,kvseqlen } = //{batch, headcount, K,N}\n",
    "batchSize=2\n",
    "headcount=8\n",
    "seqlen=256\n",
    "headsize=160\n",
    "kvseqlen=64\n",
    "#QTensor shape = {batch, headcount, seqlen,headsize}\n",
    "qTensorSize = batchSize * headcount * seqlen * headsize\n",
    "qTensor = np.arange(qTensorSize).reshape(batchSize, headcount, seqlen, headsize)\n",
    "print(\"qTensor.shape=\",qTensor.shape)\n",
    "#KTensor shape = {batch, headcount, headsize, kvseqlen}\n",
    "kTensorSize = batchSize * headcount * headsize * kvseqlen\n",
    "kTensor = np.arange(kTensorSize).reshape(batchSize, headcount, headsize,kvseqlen)\n",
    "print(\"kTensor.shape=\",kTensor.shape)\n",
    "print(\"qTensor=\",qTensor)\n",
    "print(\"kTensor=\",kTensor)\n",
    "res = np.matmul(qTensor,kTensor)\n",
    "res.tofile('GemmOutTensor',sep=\" \",format=\"%s\")\n",
    "print(\"gemm result=\",res)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43ee1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "(2, 2)\n",
      "(2, 2)\n",
      "[[ 7 10]\n",
      " [15 22]]\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "a = np.array([1,2,3,4]).reshape(2,2)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = np.array([1,2,3,4]).reshape(2,2)\n",
    "print(b.shape)\n",
    "res = np.matmul(a,b)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#int q_sizes[dim_count] = {2,1,2,32}; //{batch, headcount, seqlen,headsize } = //{batch, headcount, M,K}\n",
    "#int k_sizes[dim_count] = {2,1,32,2 }; //{batch, headcount,headsize, kvseqlen } = ////{batch, headcount,K,N}\n",
    "#int out_sizes[dim_count] = { 2,1,2,2 }; // {batchSize, headCount, sequenceLength, kvSequenceLength}\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "a = np.array([1,2,3,4]).reshape(2,2)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "b = np.array([2,1]).reshape(2,1)\n",
    "print(b.shape)\n",
    "res = np.matmul(a,b)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57b303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outTensor.shape is (2, 8, 256, 160)\n",
      "outTensorBeforeTranspose write complete\n",
      "outTensorTranspose.shape is (2, 256, 8, 160)\n",
      "outTensorTranspose.strides is (1310720, 640, 163840, 4)\n",
      "outTensorAfterTranspose write complete\n"
     ]
    }
   ],
   "source": [
    "#gemm1 out transpose\n",
    "#9.\tAfter softmax, it’s time to do the second GEMM, which is a dot product of the result of Softmax and the Value tensor. \n",
    "#The result of this GEMM will have shape {batchSize, headCount, sequenceLength, vHeadSize}\n",
    "#2,8,256,160\n",
    "#10.Then, transpose the result of the second GEMM into a tensor that has shape \n",
    "#{batchSize, sequenceLength, headCount, vHeadSize}\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "batchSize = 2\n",
    "headCount = 8\n",
    "seqlen = 256\n",
    "vheadsize = 160\n",
    "\n",
    "TensorSize = batchSize*seqlen*headCount*1*vheadsize\n",
    "outTensor = np.arange(TensorSize).reshape(batchSize,headCount,seqlen, vheadsize)\n",
    "print(\"outTensor.shape is\",outTensor.shape)\n",
    "outTensor.tofile('TensorBeforeTranspose',sep=\" \",format=\"%s\")\n",
    "print(\"outTensorBeforeTranspose write complete\")\n",
    "outTensorTranspose = outTensor.transpose(0,2,1,3)\n",
    "print(\"outTensorTranspose.shape is\",outTensorTranspose.shape)\n",
    "print(\"outTensorTranspose.strides is\",outTensorTranspose.strides)\n",
    "outTensorTranspose.tofile('TensorAfterTranspose',sep=\" \",format=\"%s\")\n",
    "print(\"outTensorAfterTranspose write complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
